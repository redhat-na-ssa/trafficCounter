{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9629c-70f8-4111-9025-e544ef1d128f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35068435-9194-4e8b-bb19-f4483f9d7c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# MinIO server information\n",
    "my_minio_endpoint = str(os.getenv('AWS_S3_ENDPOINT'))  # Your MinIO server URL\n",
    "my_access_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "my_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "my_bucket_name = os.getenv('AWS_S3_BUCKET')\n",
    "object_key = 'gdotdata.zip'  # The file you want to pull\n",
    "download_path = 'trafficCounter/model/gdotdata.zip'  # Where to save the file\n",
    "\n",
    "dataset_dir = \"datasets\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742098b-e258-497f-bc9e-114cbd3b6093",
   "metadata": {},
   "source": [
    "# Pull training data\n",
    "Retrieve the training data from the Minio bucket. Be sure the bucket exists and upload the file to the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e8a82-4652-4e23-b64f-eb13f10d2bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import urllib3\n",
    "from minio.error import S3Error\n",
    "from urllib3 import PoolManager\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "# # Set the current working directory\n",
    "# os.chdir('datasets')\n",
    "\n",
    "# Suppress SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Create a custom HTTP client to allow self-signed certificates\n",
    "http_client = PoolManager(\n",
    "    cert_reqs='CERT_NONE',  # Do not verify SSL certificates\n",
    "    ssl_context=ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n",
    ")\n",
    "\n",
    "# Initialize the MinIO client with the custom HTTP client\n",
    "minio_client = Minio(\n",
    "    my_minio_endpoint.replace(\"http://\", \"\"),  # Replace with your MinIO server endpoint and port\n",
    "    access_key=my_access_key,  # Replace with your access key\n",
    "    secret_key=my_secret_key,  # Replace with your secret key\n",
    "    secure=False,  # Still using https, but with self-signed certs allowed\n",
    "    http_client=http_client  # Pass the custom HTTP client\n",
    ")\n",
    "\n",
    "# Specify the bucket name and object name\n",
    "bucket_name = my_bucket_name\n",
    "object_name = \"gdotdata.zip\"\n",
    "file_path = dataset_dir + \"/gdotdata.zip\"  # Local path where the file will be saved\n",
    "\n",
    "try:\n",
    "    # Download the object from the bucket\n",
    "    minio_client.fget_object(bucket_name, object_name, file_path)\n",
    "    print(f\"Object '{object_name}' downloaded successfully to '{file_path}'\")\n",
    "except S3Error as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7f186-318b-4a63-921b-acbadcb52080",
   "metadata": {},
   "source": [
    "# Unzip data\n",
    "Unzip the training data to the datasets directory. Aftrer unzip, it should contain\n",
    "\n",
    "classes.txt\n",
    "images\n",
    "lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffcb2a-b3d3-4053-9a6c-5937056c5332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip datasets/gdotdata.zip -d datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580b159-9ff2-4455-a069-bb2148bcd23b",
   "metadata": {},
   "source": [
    "# Distribute training data\n",
    "This Python script is used to prepare a dataset for model training by organizing images and their corresponding labels into three subsets: training, testing, and validation. The script divides the dataset according to specified proportions (e.g., 70% for training, 15% for testing, and 15% for validation). It ensures that the data is randomly shuffled and then distributed into the appropriate directories, which are crucial for training, evaluating, and validating machine learning models. After organizing the files, it provides a summary of how the data has been split, giving insights into the distribution of the dataset across the training, testing, and validation sets. This organization is essential for training robust and reliable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720ed9b-9a84-4d0b-95c4-68949c16639a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the paths for the original image and label directories\n",
    "image_dir = dataset_dir + \"/images\"\n",
    "label_dir = dataset_dir + \"/labels\"\n",
    "\n",
    "# Set the path for the training directory\n",
    "training_dir = dataset_dir + \"/training\"\n",
    "\n",
    "# Set the desired proportions for train, test, and valid sets\n",
    "train_split = 0.7\n",
    "test_split = 0.15\n",
    "valid_split = 0.15\n",
    "\n",
    "def count_files(directory):\n",
    "    file_count = 0\n",
    "    # Iterate over all files in the directory\n",
    "    for _, _, files in os.walk(directory):\n",
    "        file_count += len(files)\n",
    "    return file_count\n",
    "\n",
    "# Create the training directory\n",
    "os.makedirs(training_dir, exist_ok=True)\n",
    "\n",
    "# Create the train, test, and valid directories within the training directory\n",
    "train_dir = os.path.join(training_dir, \"train\")\n",
    "test_dir = os.path.join(training_dir, \"test\")\n",
    "valid_dir = os.path.join(training_dir, \"valid\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(valid_dir, exist_ok=True)\n",
    "\n",
    "# Create image and label directories within train, test, and valid directories\n",
    "train_image_dir = os.path.join(train_dir, \"images\")\n",
    "train_label_dir = os.path.join(train_dir, \"labels\")\n",
    "os.makedirs(train_image_dir, exist_ok=True)\n",
    "os.makedirs(train_label_dir, exist_ok=True)\n",
    "\n",
    "test_image_dir = os.path.join(test_dir, \"images\")\n",
    "test_label_dir = os.path.join(test_dir, \"labels\")\n",
    "os.makedirs(test_image_dir, exist_ok=True)\n",
    "os.makedirs(test_label_dir, exist_ok=True)\n",
    "\n",
    "valid_image_dir = os.path.join(valid_dir, \"images\")\n",
    "valid_label_dir = os.path.join(valid_dir, \"labels\")\n",
    "os.makedirs(valid_image_dir, exist_ok=True)\n",
    "os.makedirs(valid_label_dir, exist_ok=True)\n",
    "\n",
    "# Retrieve the list of image filenames\n",
    "image_filenames = os.listdir(image_dir)\n",
    "\n",
    "# Shuffle the image filenames\n",
    "random.shuffle(image_filenames)\n",
    "\n",
    "# Calculate the number of images for each set\n",
    "total_images = len(image_filenames)\n",
    "train_count = int(total_images * train_split)\n",
    "test_count = int(total_images * test_split)\n",
    "valid_count = total_images - train_count - test_count\n",
    "\n",
    "# Copy images and labels to the train directory\n",
    "for filename in image_filenames[:train_count]:\n",
    "    name, extension = os.path.splitext(filename)\n",
    "\n",
    "    src_image_path = os.path.join(image_dir, filename)\n",
    "    dest_image_path = os.path.join(train_image_dir, filename)\n",
    "    shutil.copy(src_image_path, dest_image_path)\n",
    "\n",
    "    label_filename, _ = os.path.splitext(filename)\n",
    "\n",
    "    # Use the original filename without extension\n",
    "    label_filename += \".txt\"  # Append \".txt\" extension\n",
    "    src_label_path = os.path.join(label_dir, label_filename)\n",
    "    dest_label_path = os.path.join(train_label_dir, label_filename)\n",
    "    shutil.copy(src_label_path, dest_label_path)\n",
    "\n",
    "# Copy images and labels to the test directory\n",
    "for filename in image_filenames[train_count : train_count + test_count]:\n",
    "    name, extension = os.path.splitext(filename)\n",
    "\n",
    "    src_image_path = os.path.join(image_dir, filename)\n",
    "    dest_image_path = os.path.join(test_image_dir, filename)\n",
    "    shutil.copy(src_image_path, dest_image_path)\n",
    "\n",
    "    label_filename, _ = os.path.splitext(filename)\n",
    "\n",
    "    # Use the original filename without extension\n",
    "    label_filename += \".txt\"  # Append \".txt\" extension\n",
    "    src_label_path = os.path.join(label_dir, label_filename)\n",
    "    dest_label_path = os.path.join(test_label_dir, label_filename)\n",
    "    shutil.copy(src_label_path, dest_label_path)\n",
    "\n",
    "# Copy images and labels to the valid directory\n",
    "for filename in image_filenames[train_count + test_count :]:\n",
    "    name, extension = os.path.splitext(filename)\n",
    "\n",
    "    src_image_path = os.path.join(image_dir, filename)\n",
    "    dest_image_path = os.path.join(valid_image_dir, filename)\n",
    "    shutil.copy(src_image_path, dest_image_path)\n",
    "\n",
    "    label_filename, _ = os.path.splitext(filename)\n",
    "\n",
    "    # Use the original filename without extension\n",
    "    label_filename += \".txt\"  # Append \".txt\" extension\n",
    "    src_label_path = os.path.join(label_dir, label_filename)\n",
    "    dest_label_path = os.path.join(valid_label_dir, label_filename)\n",
    "    shutil.copy(src_label_path, dest_label_path)\n",
    "\n",
    "print(\"Dataset files distributed to folders: test, train, and valid\")\n",
    "\n",
    "directory_path = dataset_dir + \"/training/test/images\"\n",
    "num_test = count_files(directory_path)\n",
    "directory_path = dataset_dir + \"/training/train/images\"\n",
    "num_train = count_files(directory_path)\n",
    "directory_path = dataset_dir + \"/training/valid/images\"\n",
    "num_valid = count_files(directory_path)\n",
    "\n",
    "num_total = num_test + num_train + num_valid\n",
    "\n",
    "print(\"File count - Test:       [\" + format(num_test/num_total, \".0%\") + \"] \" + str(num_test))\n",
    "print(\"File count - Train:      [\" + format(num_train/num_total, \".0%\") + \"] \" + str(num_train))\n",
    "print(\"File count - Validation: [\" + format(num_valid/num_total, \".0%\") + \"] \" + str(num_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fb596-095f-49d8-ad9d-1a0cb78a29ee",
   "metadata": {},
   "source": [
    "# Create the class file\n",
    "This Python script generates a YAML configuration file that is essential for training a machine learning model, particularly in tasks like object detection. It reads class names from a text file and organizes them along with paths to the training, validation, and testing datasets. The resulting YAML file provides a structured configuration that the model training framework uses to locate the dataset, understand the number of classes, and apply the correct labels during training, validation, and testing phases. This setup is crucial for ensuring the model is trained on the correct data and knows how to interpret the classes it will learn to recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744c40b-06e3-46e4-ad40-dab682c574af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Read lines from the text file\n",
    "with open(dataset_dir + '/classes.txt', 'r') as file:\n",
    "    lines = file.read().splitlines()\n",
    "\n",
    "# Prepare the data for the YAML file\n",
    "yaml_data = {\n",
    "    'path': 'training',\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'nc': len(lines),\n",
    "    'names': lines\n",
    "}\n",
    "\n",
    "# Write to a YAML file\n",
    "with open(dataset_dir + '/gdot.yaml', 'w') as yaml_file:\n",
    "    yaml.dump(yaml_data, yaml_file, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"YAML file 'gdot.yaml' created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a98559-c481-4766-88a2-9efd36f996a4",
   "metadata": {},
   "source": [
    "# Set ultralytics config\n",
    "This step is necessary to set proper defaults for Ultralytics to pickup proper locations for training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c2afc-b9be-4f10-aa6f-6b96de607c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Path to the settings.yaml file\n",
    "config_file = \"/opt/app-root/src/.config/Ultralytics/settings.yaml\"\n",
    "custom_datasets_dir = \"/opt/app-root/src/trafficCounter/model/datasets\"\n",
    "\n",
    "# Ensure the settings.yaml file exists\n",
    "if not os.path.exists(config_file):\n",
    "    os.makedirs(os.path.dirname(config_file), exist_ok=True)\n",
    "    settings = {\n",
    "        'settings_version': '0.0.4',\n",
    "        'datasets_dir': custom_datasets_dir,\n",
    "        'weights_dir': 'weights',\n",
    "        'runs_dir': 'runs',\n",
    "        'uuid': os.popen('uuidgen').read().strip(),\n",
    "        'sync': True,\n",
    "        'api_key': '',\n",
    "        'openai_api_key': '',\n",
    "        'clearml': True,\n",
    "        'comet': True,\n",
    "        'dvc': True,\n",
    "        'hub': True,\n",
    "        'mlflow': True,\n",
    "        'neptune': True,\n",
    "        'raytune': True,\n",
    "        'tensorboard': True,\n",
    "        'wandb': True,\n",
    "    }\n",
    "else:\n",
    "    # Load existing settings.yaml\n",
    "    with open(config_file, 'r') as file:\n",
    "        settings = yaml.safe_load(file)\n",
    "    \n",
    "    # Update the datasets_dir\n",
    "    settings['datasets_dir'] = custom_datasets_dir\n",
    "\n",
    "# Save the updated settings.yaml\n",
    "with open(config_file, 'w') as file:\n",
    "    yaml.dump(settings, file)\n",
    "\n",
    "print(f\"datasets_dir set to {custom_datasets_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13146bf0-bcce-4cb4-a118-892801bdcdc8",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "## Command Overview\n",
    "\n",
    "This command initiates the training of a YOLOv8 model with the following parameters:\n",
    "\n",
    "- **`source=datasets/training`**: Specifies the directory containing the training dataset.\n",
    "- **`model=yolov8n.pt`**: Uses the pre-trained YOLOv8n model as the starting point for training.\n",
    "- **`epochs=100`**: Trains the model for 100 epochs (iterations over the entire dataset).\n",
    "- **`batch=16`**: Processes the data in batches of 16 images at a time during training.\n",
    "- **`data=datasets/gdot.yaml`**: Points to a YAML file that defines the dataset structure, including paths to the train, validation, and test sets, and the class names.\n",
    "- **`project=runs`**: Saves the training results, including model weights and logs, in a directory named `runs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf4f3f4-5310-4d7b-8829-9be289a0a7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!yolo train source=datasets/training model=yolov8n.pt epochs=100 batch=16 data=datasets/gdot.yaml project=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc395b-afdd-4b33-8da0-194a4bf1381d",
   "metadata": {},
   "source": [
    "# Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3829fe8-c605-4755-b049-e1c25e9d5470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Open an image file\n",
    "img = Image.open('runs/train/results.png')\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3ac00-f5ab-47e0-bea3-af53ee5471ba",
   "metadata": {},
   "source": [
    "# Export the new model\n",
    "## Command Overview\n",
    "\n",
    "This command exports a trained YOLOv8 model to a different format with the following parameters:\n",
    "\n",
    "- **`model=\"runs/train/weights/best.pt\"`**: Specifies the path to the trained YOLOv8 model file (`best.pt`) that you want to export.\n",
    "- **`format=onnx`**: Converts the model to the ONNX format, which is commonly used for deploying models across different platforms.\n",
    "- **`imgsz=224,128`**: Specifies the input image size for the exported model, setting it to a width of 224 pixels and a height of 128 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7ec69-106d-4db3-b96f-51fc60c41013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!yolo export model=\"runs/train/weights/best.pt\" format=onnx imgsz=224,128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa081d34-cc96-4fec-8f65-a9c8dc3d124d",
   "metadata": {},
   "source": [
    "# Copy model to bucket\n",
    "Upload the new model to a Minio bucket for consumption by other services/serving components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e848f3-b21c-48ea-b893-173a877c2e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import urllib3\n",
    "\n",
    "# Suppress SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Create a custom HTTP client to allow self-signed certificates\n",
    "http_client = PoolManager(\n",
    "    cert_reqs='CERT_NONE',  # Do not verify SSL certificates\n",
    "    ssl_context=ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n",
    ")\n",
    "\n",
    "# Initialize the MinIO client with the custom HTTP client\n",
    "minio_client = Minio(\n",
    "    my_minio_endpoint.replace(\"http://\", \"\"),  # Replace with your MinIO server endpoint and port\n",
    "    access_key=my_access_key,  # Replace with your access key\n",
    "    secret_key=my_secret_key,  # Replace with your secret key\n",
    "    secure=False,  # Still using https, but with self-signed certs allowed\n",
    "    http_client=http_client  # Pass the custom HTTP client\n",
    ")\n",
    "\n",
    "\n",
    "# Set file and bucket details\n",
    "file_path = \"runs/train/weights/best.onnx\"  # Local file path\n",
    "bucket_name = my_bucket_name\n",
    "object_name = \"best.onnx\"  # Name to save in the bucket\n",
    "\n",
    "try:\n",
    "    # Upload the file to the MinIO bucket\n",
    "    minio_client.fput_object(\n",
    "        bucket_name, object_name, file_path\n",
    "    )\n",
    "    print(f\"File '{file_path}' uploaded to bucket '{bucket_name}' as '{object_name}'.\")\n",
    "\n",
    "except S3Error as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942d345-6a7a-4020-92a0-104b47db1bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
